{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11_QtarKSjPf"
   },
   "source": [
    "### Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S_p54xNSTq_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3hxKJ822wO2"
   },
   "source": [
    "You have to install the urduhack on your system or colab by using given command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIhS278lWQOv"
   },
   "outputs": [],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX4IBqUzWHMp"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import urduhack\n",
    "from urduhack.tokenization import sentence_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XChiPrRsSwjz"
   },
   "source": [
    "#### Reading Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxjaBCgOWYKt"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'urdu-train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c53c68d3dbb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urdu-train.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'urdu-train.txt'"
     ]
    }
   ],
   "source": [
    "with open('urdu-train.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(reader)\n",
    "    train_data = list(reader)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gaElHuGS4Qb"
   },
   "source": [
    "#### Reading Labeled File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnPi90zhegAM"
   },
   "outputs": [],
   "source": [
    "with open('urdu-test.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_data = list(reader)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIEIcy0RY0vx"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ew0jEnnuY9W0"
   },
   "source": [
    "##### For Single Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVxXoZtCTJJN"
   },
   "outputs": [],
   "source": [
    "def acc(test_text,label_text):\n",
    "    \n",
    "    correct=0\n",
    "    for i in label_text:\n",
    "        for j in test_text:\n",
    "            lb=i.split(' ')\n",
    "            ts=j.split(' ')\n",
    "            if(len(lb)>2 and len(ts)>2):\n",
    "                if(lb[-1]==ts[-1] and lb[-2]==ts[-2] and lb[-3]==ts[-3]):\n",
    "                    correct=correct+1\n",
    "            else:\n",
    "                return('Error')\n",
    "    accuracy=(float(correct)/float(len(label_text)))*100\n",
    "      # print('Accuracy\\t=\\t'+str(accuracy)+'%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2kkMUO4ZFDs"
   },
   "source": [
    "##### For Multiple Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2V1WzM4-Q4bY"
   },
   "outputs": [],
   "source": [
    "def Accuracy(train,test):\n",
    "    total=0\n",
    "    e=0\n",
    "    for i in range(len(test)):\n",
    "        train_text = sentence_tokenizer(train[i][0])\n",
    "        label_text = test[i][0].split('۔')\n",
    "        tmp=acc(train_text,label_text)\n",
    "        if(tmp!='Error'):\n",
    "            total=total+tmp\n",
    "        else:\n",
    "            e=e+1\n",
    "\n",
    "    accuracy=(float(total)/float(len(test)-e))\n",
    "    print('Accuracy\\t=\\t'+str(accuracy)+'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNpYuOgVXSfW"
   },
   "outputs": [],
   "source": [
    "test = train_data[0][0]\n",
    "label = test_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zpiosslj6Q_"
   },
   "source": [
    "#### URDUhack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15HJy4e1XU1c"
   },
   "outputs": [],
   "source": [
    "test_text = sentence_tokenizer(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhzL1srwTmOG"
   },
   "source": [
    "Code here.............................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72Nh1JgFTtTp"
   },
   "outputs": [],
   "source": [
    "#For training data.\n",
    "\n",
    "def split_using_delimeter(_str, separator):\n",
    "    \"\"\"Replace end of sentence with separator\"\"\"\n",
    "    if not _str:\n",
    "        return []\n",
    "    max_p = chr(ord(max(_str)) + 1)\n",
    "    return _str.replace(separator, separator + max_p).split(max_p)\n",
    "\n",
    "def get_endwords_and_conjunctions(text):\n",
    "    endwords = ['رہے','کیجیے', 'کیجئے', 'گئیں','دیگی','کئے','کرنا','ہوں','ملینگے','جائے','لگا','کہا','کیں','لگی','تھیں', 'تھا', 'دیگی', 'رکھی', 'تھی', 'رکھا', 'ہوی', 'تھے', 'چاہیے','ملیںگی', 'گا', 'کرنا', 'گیا', 'دیا', 'ہونگے', 'گی', 'دیگا', 'ہیں', 'لیں', 'ہے',\n",
    "                    ]\n",
    "    conjunctions = ['یا','اگرچہ','یعنی','گویا','جبکہ','جب کہ','جن','تو','اور','جسے', 'تاہم', 'جس', 'بلکہ', 'مگر', 'کیونکہ', 'پر',\n",
    "                    ]\n",
    "    \n",
    "    sentences = split_using_delimeter(text, '۔')\n",
    "    \n",
    "    for _sen in sentences:\n",
    "        _sen = _sen.replace('۔', '')\n",
    "        _sen = _sen.replace('؟', '')\n",
    "        \n",
    "        _sen = _sen.split() \n",
    "        if _sen:     \n",
    "            endwords.append(_sen[-1])   #Picking the words before delimiter \n",
    "            \n",
    "\n",
    "        for index, word in enumerate(_sen):\n",
    "            if word in endwords and len(_sen)-1!=index:   #If word is an endword, length of sentence is not null and word+1 is not an endword, then word+1 is a conjunction\n",
    "                if _sen[index+1] not in endwords:\n",
    "                    conjunctions.append(_sen[index+1])\n",
    "                    \n",
    "    return set(endwords), set(conjunctions)\n",
    "\n",
    "endwords, conjunctions = get_endwords_and_conjunctions(label)\n",
    "\n",
    "# print(endwords)\n",
    "# print(conjunctions)\n",
    "\n",
    "def lets_segment(text):\n",
    "    sentences = split_using_delimeter(text, '۔')\n",
    "\n",
    "    for each in sentences:\n",
    "        each = each.split()\n",
    "\n",
    "    allsentences = []\n",
    "    newsen= []\n",
    "    for index, word in enumerate(each):\n",
    "        if len(each)-1!=index:\n",
    "            if (word not in endwords) or (word in endwords and (each[index+1] in conjunctions or each[index+1] in endwords)):   #Append words if sentence not finished\n",
    "                newsen.append(word)\n",
    "            else:\n",
    "                newsen.append(word)          #Endword of the sentence reached, so append\n",
    "                newsen = ' '.join(newsen)    #Stringify\n",
    "                allsentences.append(newsen)  #Add to list of sentences\n",
    "                newsen = []\n",
    "                \n",
    "    return allsentences\n",
    "\n",
    "\n",
    "train_data = lets_segment(test)\n",
    "\n",
    "# seg = '۔ '.join(train_data)\n",
    "# print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhXFBUoHkEDH"
   },
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2YkvsjHkVff"
   },
   "outputs": [],
   "source": [
    "label_text=label.split('۔')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZ83jAJ3S-uR"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OyFZPRIGlWNy",
    "outputId": "acc0337b-66a1-4fd5-a803-266c7b5ecc67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.07407407407408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Urduhack accuracy\n",
    "acc(test_text,label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.48148148148148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Custom algorithm accuracy\n",
    "acc(train_data, label_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
